<?xml version="1.0" encoding="UTF-8" ?><ChoregrapheProject xmlns="http://www.aldebaran-robotics.com/schema/choregraphe/project.xsd" xar_version="3"><Box name="root" id="-1" localization="8" tooltip="Root box of Choregraphe&apos;s behavior. Highest level possible." x="0" y="0"><bitmap>media/images/box/root.png</bitmap><script language="4"><content><![CDATA[]]></content></script><Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" /><Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" /><Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" /><Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" /><Timeline enable="0"><BehaviorLayer name="behavior_layer1"><BehaviorKeyframe name="keyframe1" index="1"><Diagram><Box name="CAIR Client" id="1" localization="8" tooltip="This box contains a Python script that allows the robot to connect to the CAIR Cloud. &#x0A;This behavior uses ASR2 to understand the user&apos;s input and sends it to the CAIR Cloud; the response of the server is then used to make the robot reply to the user and, if present, execute the required action by calling one of the other implemented behaviors." x="236" y="109"><bitmap>media/images/box/box-python-script.png</bitmap><script language="4"><content><![CDATA[import qi
import sys
sys.path.append("/data/home/nao/.local/share/PackageManager/apps/dialoguemanager/libs/")
sys.path.append("/data/home/nao/.local/share/PackageManager/apps/dialoguemanager/")
from naoqi import ALProxy
from numpy.random import choice
import threading
import requests
import argparse
import pickle
import time
import json
import os
import re
import zlib
import carlo


class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self)
        self.utils = carlo.Utils(self.logger.info)
        self.isAlive = True
        self.memory = ALProxy("ALMemory")
        self.sASR = ALProxy("ASR2")
        self.speech_reco_event = "Audio/RecognizedWords"
        self.al = ALProxy("ALAutonomousLife")
        self.motion = ALProxy("ALMotion")
        self.tts = ALProxy("ALTextToSpeech")
        self.animated_speech = ALProxy("ALAnimatedSpeech")
        self.configuration = {"bodyLanguageMode": "contextual"}
        self.utils.setAutonomousAbilities(True, True, True, True, True)

        self.behavior_manager = ALProxy("ALBehaviorManager")
        self.audio_device = ALProxy("ALAudioDevice")
        self.tablet = True
        # Store the number of people in front of the robot, coming from the sensor readings
        self.people = 0
        self.state_file_path = "/data/home/nao/.local/share/PackageManager/apps/dialoguemanager/state.txt"

        try:
            self.tablet_service = ALProxy("ALTabletService")
        except:
            self.tablet = False

        try:
            self.voice_speed = "\\RSPD=" + str(self.memory.getData("CAIR/voice_speed")) + "\\"
        except:
            self.memory.insertData("CAIR/voice_speed", 80)
            self.voice_speed = "\\RSPD=80\\"

        self.audio_device = ALProxy("ALAudioDevice")
        self.audio_device.setOutputVolume(60)

        self.asr_service = ALProxy("ALSpeechRecognition")
        self.asr_service.setAudioExpression(False)

    def onLoad(self):
        #put initialization code here
        pass

    def onUnload(self):
        #put clean-up code here
        pass

    def get_sensor_data_thread(self):
        while self.isAlive:
            self.logger.info("Thread reading sensor data")
            self.people = self.memory.getData("SensorData/People")
            self.logger.info("ciao")
            data = {"sentence": "", "client_state": "", "sensor_data": {"people": self.people}}
            encoded_data = json.dumps(data).encode('utf-8')
            compressed_data = zlib.compress(encoded_data)
            self.logger.info("ciao1")
            try:
                response = requests.get(BASE, data=compressed_data, verify=False)
                self.logger.info(response)
                self.logger.info("Server response:" + response.json()['reply'])
            except:
                self.logger.info("ciao2")
            time.sleep(1)
        self.logger.info("Exiting sensor data thread")
        self.behavior_manager.stopBehavior("sensordata/people_perception")

    def onInput_onStart(self):
        # Location of the API (the server it is running on)
        server_IP = "130.251.13.128"
        BASE = "http://" + server_IP + ":50000/CAIR_hub"
        self.previous_sentence = ""
        self.not_installed_behavior = "I'm sorry, I can't perform this task because the behavior is not installed."
        self.repeat = False

        try:
            if self.al.getState() != "disabled":
                self.animated_speech.say(self.voice_speed + "Give me a moment. I need to change some of my settings before we start.", self.configuration)
                self.al.setState("disabled")
        except:
            pass

        self.motion.wakeUp()

        # With Pepper robot, preload the images to be set on the tablet during the conversation
        if self.tablet:
            self.tablet_service.preLoadImage("http://" + self.tablet_service.robotIp() + "/apps/dialoguemanager/img/DialogueMode.png")
            self.tablet_service.preLoadImage("http://" + self.tablet_service.robotIp() + "/apps/dialoguemanager/img/ExecutionMode.png")
            self.tablet_service.preLoadImage("http://" + self.tablet_service.robotIp() + "/apps/dialoguemanager/img/PrivacyMode.png")
            self.tablet_service.showImage("http://" + self.tablet_service.robotIp() + "/apps/dialoguemanager/img/DialogueMode.png")


        # If the file with the credentials exists, read the ID and use it during the requests to the Cloud
        if not os.path.exists(self.state_file_path):
            resp = requests.put(BASE, verify=False)
            state = resp.json()["dialogue_state"]
            with open(self.state_file_path, 'wb') as f:
                pickle.dump(state, f)
            self.animated_speech.say(self.voice_speed + "Welcome to CAIR!" + str(resp.json()["reply"]), self.configuration)
            self.previous_sentence = "Welcome to CAIR!" + str(resp.json()["reply"])
        else:
            self.animated_speech.say(self.voice_speed + "I missed you! What would you like to talk about?", self.configuration)
            self.previous_sentence = "I missed you! What would you like to talk about?"

        if self.behavior_manager.isBehaviorInstalled("sensordata/people_perception"):
            if not self.behavior_manager.isBehaviorRunning("sensordata/people_perception"):
                self.behavior_manager.runBehavior("sensordata/people_perception")

        sensor_data_thread = threading.Thread(target=self.get_sensor_data_thread)
        sensor_data_thread.start()

        # Loop until the user wants to stop the conversation
        while self.isAlive:
            time.sleep(0.1)
            self.sASR.startReco("English", False, True)
            self.utils.setAutonomousAbilities(True, True, True, True, True)

            start_time_silence = time.time() + 1800

            sentence = ""
            reco_speech = self.memory.getData(self.speech_reco_event)
            self.logger.info(str(reco_speech))
            stop = False

            while True:
                time.sleep(0.1)
                while not reco_speech:
                    time.sleep(0.1)
                    if sentence and time.time() - start_time_silence > 0.1:
                        self.sASR.stopReco()
                        self.utils.setAutonomousAbilities(False, True, True, True, True)
                        stop = True
                        break
                    reco_speech = self.memory.getData(self.speech_reco_event)
                    print reco_speech
                sys.stdout.flush()
                if stop:
                    break
                if sentence == "":
                    sep = ""
                else:
                    sep = " "

                # If something has been recognized
                if reco_speech:
                    sentence = sentence + sep + reco_speech[0][0]
                start_time_silence = time.time()
                self.memory.insertData(self.speech_reco_event, [])
                reco_speech = self.memory.getData(self.speech_reco_event)

                exit_keywords = ["stop talking", "quit the application"]
                repeat_keywords = ["repeat", "say it again"]
                # Reset repeat to false, otherwise it will always repeat the previous sentence
                self.repeat = False

                # If the user said one of the "Exit Application keywords"
                if sentence.lower() in exit_keywords:
                    self.isAlive = False
                    self.animated_speech.say(self.voice_speed + "Ok, thank you for talking with me! Goodbye.", self.configuration)
                    self.memory.insertData(self.speech_reco_event, [])
                    self.onInput_onStop()
                # If the user said a Repeat keyword
                elif sentence.lower() in repeat_keywords:
                    # If a previous sentence to repeat exists
                    if self.previous_sentence:
                        self.repeat = True
                        self.animated_speech.say(self.voice_speed + "Sure. I said: " + str(self.previous_sentence), self.configuration)
                    else:
                        self.animated_speech.say(self.voice_speed + "I'm sorry, I don't have anything to repeat.", self.configuration)

            # If the user does not want the robot to repeat something, send the sentence to the Cloud and process the response
            if not self.repeat:
                self.logger.info(str(sentence))
                self.memory.insertData(self.speech_reco_event, [])
                with open(self.state_file_path, 'rb') as f:
                    dialogue_state = pickle.load(f)
                data = {"sentence": sentence, "dialogue_state": dialogue_state, "sensor_data": {"people": self.people}}
                encoded_data = json.dumps(data).encode('utf-8')
                compressed_data = zlib.compress(encoded_data)
                response = requests.get(BASE, data=compressed_data, verify=False)
                # Retrieve and store the updated client state
                dialogue_state = response.json()["dialogue_state"]
                with open(self.state_file_path, 'wb') as f:
                    dialogue_state = pickle.dump(dialogue_state, f)

                intent_reply = response.json()['intent_reply']
                plan = response.json()['plan']
                reply = response.json()['reply']

                # If there is no plan to execute, just say the reply of the server
                if not plan:
                    reply = intent_reply + " " + reply
                    self.animated_speech.say(self.voice_speed + str(reply), self.configuration)
                    self.previous_sentence = str(reply)
                    self.logger.info(str(reply))
                # If there is a plan to execute, parse the actions of which it is composed and perform them
                else:
                    self.logger.info(str(plan))
                    # If the intent has a reply, say that before performing the action
                    if intent_reply:
                        self.animated_speech.say(self.voice_speed + str(intent_reply), self.configuration)
                        self.logger.info(str(intent_reply))

                    plan_items = plan.split("#")[1:]
                    self.logger.info(plan_items)
                    # For each action in the plan, check which action is it and execute it (if the corresponding behavior is installed)
                    for item in plan_items:
                        item = item.encode('utf-8')
                        self.logger.info(item)
                        action = re.findall("action=(\w+)", item)[0]
                        self.logger.info(str(action))
                        self.utils.perform_action(action)

                    # Once the execution of the plan is finished, set the dialogue mode image on the tablet
                    if self.tablet:
                        self.tablet_service.showImage("http://" + self.tablet_service.robotIp() + "/apps/dialoguemanager/img/DialogueMode.png")

                    # If there is a reply from the Cloud, say it after the plan has been executed
                    if reply:
                        self.animated_speech.say(self.voice_speed + str(reply), self.configuration)
                        self.previous_sentence = reply

    def onInput_onStop(self):
        if self.tablet:
            self.tablet_service.hideImage()
        self.al.setState("interactive")
        self.utils.setAutonomousAbilities(True, True, True, True, True)
        # If present, delete the transformation - if the robot moves outside CAIR it is no more valid
        try:
            self.memory.removeData("CAIR/transformation_matrix")
            self.memory.removeData("CAIR/theta")
        except:
            self.logger.info("No transformation to delete in memory.")
            pass
        self.onStopped()
        self.onUnload()
        sys.exit(0)]]></content></script><Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" /><Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" /><Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" /><Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" /></Box><Link inputowner="0" indexofinput="4" outputowner="1" indexofoutput="4" /><Link inputowner="1" indexofinput="2" outputowner="0" indexofoutput="2" /></Diagram></BehaviorKeyframe></BehaviorLayer></Timeline></Box></ChoregrapheProject>